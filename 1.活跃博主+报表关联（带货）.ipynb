{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1.读取CSV文件"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import dask\n",
    "import dask.dataframe as dd\n",
    "from datetime import timedelta\n",
    "\n",
    "file_path = r'.\\data'\n",
    "\n",
    "#dask处理大型文件，相较于pandas可极大节省时间。\n",
    "#因为表中存在URL链接，因此解码容易出错，忽略。\n",
    "video_data = dd.read_csv(file_path+r'\\video_data.csv',encoding='GB18030',encoding_errors='ignore',\n",
    "usecols=['category', 'author_id','name','title',\n",
    "'rid','comment','likes','share','duration','createtime',\n",
    "'product','sales','volume','aweme_url','collect_count',\n",
    "'download_count','forward_count','play_count','product_count',],\n",
    "dtype={'category':'string','author_id': 'string','name': 'string','title': 'string',\n",
    "'rid': 'string','comment': 'string','likes': 'float32','share':'float32','duration': 'float32',\n",
    "'product': 'string','sales':'float32','volume':'float32','aweme_url':'string','collect_count':'float32',\n",
    "'download_count': 'float32','forward_count': 'float32','play_count':'float32','product_count': 'float32'\n",
    "})\n",
    "\n",
    "print(video_data.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2.选取并保留活跃博主"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_week_begin = pd.Timestamp(\"2023-06-05\")\n",
    "start_week_end = start_week_begin + timedelta(days=7)\n",
    "end_week_begin = pd.Timestamp(\"2024-06-24\")\n",
    "end_week_end = end_week_begin + timedelta(days=7)\n",
    "\n",
    "authors_info = dd.read_csv(file_path+r'\\authors_info.csv',encoding='UTF-8')\n",
    "print(authors_info[\"video_earliest_createtime\"].head())\n",
    "\n",
    "authors_info[\"video_earliest_createtime\"] = dd.to_datetime(\n",
    "    authors_info[\"video_earliest_createtime\"],\n",
    "    format=\"%Y-%m-%d %H:%M:%S\",\n",
    "    errors=\"coerce\")\n",
    "authors_info[\"video_latest_createtime\"] = dd.to_datetime(\n",
    "    authors_info[\"video_latest_createtime\"],\n",
    "    format=\"%Y-%m-%d %H:%M:%S\",\n",
    "    errors=\"coerce\")\n",
    "\n",
    "start_week_mask = (authors_info[\"video_earliest_createtime\"] >= start_week_begin) & \\\n",
    "                  (authors_info[\"video_earliest_createtime\"] <= start_week_end)\n",
    "end_week_mask = (authors_info[\"video_latest_createtime\"] >= end_week_begin) & \\\n",
    "                (authors_info[\"video_latest_createtime\"] <= end_week_end)\n",
    "\n",
    "start_week_authors = authors_info.loc[start_week_mask, \"author_id\"].unique()\n",
    "end_week_authors = authors_info.loc[end_week_mask, \"author_id\"].unique()\n",
    "\n",
    "active_authors = set(start_week_authors) & set(end_week_authors)#得到活跃博主清单\n",
    "\n",
    "print(f\"活跃博主数量: {len(active_authors)}\")\n",
    "print(\"活跃博主ID示例:\", list(active_authors)[:5])\n",
    "\n",
    "active_mask = video_data[\"author_id\"].isin(active_authors)\n",
    "video_data = video_data[active_mask]#筛选活跃博主\n",
    "\n",
    "print(\"活跃博主数据示例:\")\n",
    "print(video_data.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3.以video_data为主表进行数据合并操作"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "video_data = video_data[video_data.sales != 0]\n",
    "\n",
    "category_columns = [\n",
    "    'v11_category_big', 'v11_category_big_id',\n",
    "    'v11_category_first', 'v11_category_first_id',\n",
    "    'v11_category_second', 'v11_category_second_id',\n",
    "    'v11_category_third', 'v11_category_third_id',\n",
    "    'v11_category_fourth', 'v11_category_fourth_id']\n",
    "\n",
    "product_data = dd.read_csv(\n",
    "    file_path+r'\\product_data.csv',\n",
    "    usecols=['volume_text','brand_name','product_id','product_title'] + category_columns,\n",
    "    dtype={'volume_text':'string','brand_name':'string',\n",
    "    'product_id':'string','product_title':'string',\n",
    "    'v11_category_big':'category',\n",
    "    'v11_category_big_id':'int16',\n",
    "    'v11_category_first':'category',\n",
    "    'v11_category_first_id':'int16',\n",
    "    'v11_category_second':'category',\n",
    "    'v11_category_second_id':'int16',\n",
    "    'v11_category_third':'category',\n",
    "    'v11_category_third_id':'int16',\n",
    "    'v11_category_fourth':'category',\n",
    "    'v11_category_fourth_id':'int16'})\n",
    "\n",
    "def clean_volume_text(text):\n",
    "    if pd.isna(text):\n",
    "        return 0\n",
    "\n",
    "    #清晰数据中的引号与逗号，以实现后续的float格式转换\n",
    "    text = str(text).replace(\"'\", \"\").replace('\"', '')\n",
    "    text = text.replace(\",\", \"\")\n",
    "\n",
    "    try:\n",
    "        return float(text)\n",
    "    except ValueError:\n",
    "        return 0\n",
    "\n",
    "\n",
    "product_data['volume_numeric'] = product_data['volume_text'].map(clean_volume_text, meta=('volume_text', 'float64'))\n",
    "\n",
    "#首先获取每个product_title的最大volume_numeric\n",
    "max_volume_per_title = product_data.groupby('product_title')['volume_numeric'].max().reset_index()\n",
    "\n",
    "#然后与原始数据合并，只保留那些volume_numeric等于最大值的行\n",
    "product_data_deduplicated = product_data.merge(\n",
    "    max_volume_per_title,\n",
    "    on=['product_title', 'volume_numeric'],\n",
    "    how='inner')\n",
    "\n",
    "merged_dask = video_data.merge(\n",
    "    product_data_deduplicated[['volume_text','brand_name','product_id','product_title'] + category_columns],\n",
    "    left_on='product',\n",
    "    right_on='product_title',\n",
    "    how='left').drop(columns=['product_title'])\n",
    "\n",
    "print(merged_dask.head())\n",
    "\n",
    "fan_trend = dd.read_csv(file_path+r'\\fan_trend.csv',encoding='UTF-8')\n",
    "\n",
    "merged_dask['createtime'] = dd.to_datetime(merged_dask['createtime'], \n",
    "                                               format='%Y/%m/%d %H:%M', \n",
    "                                                errors='coerce')\n",
    "fan_trend['time_node'] = dd.to_datetime(fan_trend['time_node'], \n",
    "                                              format='%Y-%m-%d %H:%M:%S', \n",
    "                                              errors='coerce')\n",
    "merged_dask = merged_dask.assign(\n",
    "    createtime = merged_dask['createtime'].astype('datetime64[ns]'))\n",
    "fan_trend = fan_trend.assign(\n",
    "    time_node = fan_trend['time_node'].astype('datetime64[ns]'))\n",
    "\n",
    "merged_dask['date_only'] = dd.to_datetime(merged_dask['createtime']).dt.date\n",
    "fan_trend['date_only'] = dd.to_datetime(fan_trend['time_node']).dt.date\n",
    "merged_dask = merged_dask.assign(\n",
    "    date_only = merged_dask['date_only'].astype('datetime64[ns]'))#修正日期格式\n",
    "fan_trend = fan_trend.assign(\n",
    "    date_only = fan_trend['date_only'].astype('datetime64[ns]'))#修正日期格式\n",
    "\n",
    "merged_dask1 = merged_dask.merge(\n",
    "    fan_trend[['cid','date_only','follower_count']],\n",
    "    left_on=['author_id','date_only'],\n",
    "    right_on=['cid', 'date_only'],\n",
    "    how='left'\n",
    ").drop(columns=['cid', 'date_only'])\n",
    "\n",
    "print(merged_dask1.head())\n",
    "\n",
    "merged_dask2 = merged_dask1.merge(\n",
    "    authors_info[['author_id','category']],\n",
    "    left_on=['author_id'],\n",
    "    right_on=['author_id'],\n",
    "    how='left')\n",
    "\n",
    "#删除其他表，大幅释放内存\n",
    "del merged_dask\n",
    "del merged_dask1\n",
    "del video_data\n",
    "del product_data\n",
    "del fan_trend\n",
    "\n",
    "print(merged_dask2.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4.保存文件"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#将多分区整合为单一文件，提高后续阅读效率\n",
    "merged_dask2.to_csv(file_path+'\\with_sales.csv',index=False,single_file=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
